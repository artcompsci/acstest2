= Populating Velocity Space

== Choosing a Velocity

*Alice*: Now that we completely understand how you choose the radial distance
for a particle, there is only one thing left to do: to choose the magnitude of
its velocity.  In your code you had a more complicated construction than the
one-liner you used for the position.  For the velocity you wrote:

 :include: .mkplummer1.rb-8

You are throwing dice a number of times, until some criterion is satisfied,
and then you move on to a new one-liner that gives you the value for the
variable +velocity+, which is the magnitude of the velocity.

*Bob*: Here is the recipe for that part.  Aarseth <i>et al.</i> start
with the observation that the maximum velocity allowed at a radius _r_
is the escape velocity <tex>$v_e(r)$</tex>, itself a function that
depends on the radius.  The escape velocity can be determined by
requiring that a particle at radius _r_ has exactly zero total energy,
<i>i.e.</i> its kinetic energy is just enough for a parabolic escape
to infinity.  Since the potential energy for a test particle moving in
Plummer's model is given as

:equation:
\Phi(r) \, = \, -\, {1\over \,\left(r^2 + 1\right)^{1/2}}

per unit mass of the test particle, we can equate that to the kinetic
energy, also per unit mass, of a particle moving at the escape
velocity:

:equation:
\half \left\{v_e(r)\right\}^2 \, + \, \Phi(r) \, = \, 0 \, \Rightarrow

:equation:
\left\{v_e(r)\right\}^2  \, = \, 2\left(r^2 + 1\right)^{-1/2} \, \Rightarrow

:equation:
\label{escapevelocity}
v_e(r)  \, = \, \sqrt{2} \left(r^2 + 1\right)^{-1/4}

This is the maximum velocity allowed at radius _r_, and we also know
that the minimum velocity at radius _r_ is zero.  The question is:
what is the probability distribution for <tex>$0\le v \le v_e$</tex>.

*Alice*: I see that you are using again the choice of units given by
Aarseth <i>et al.</i>, where <tex>$G=M=a=1$</tex>.

*Bob*: Yes, they are by far the most convenient, they save time when
writing down and manipulating the equations, and they make it also less
likely to make mistakes.

*Alice*: I'm not sure about the last part of what you said.  The advantage
of keeping the full physical quantities is that you always have a few extra
checks you can make, at the end: if the physical dimensions of length, mass
and time are not exactly the same, at the left and right hand side of an
equation, the equation must be wrong.  If you work only with dimensionless
quantities, you loose that advantage.

*Bob*: I don't consider it an advantage to have to do so much more work that
you are likely to make more mistakes, so that you can then happily catch them.
But clearly we are talking about matters of taste, and we have already decided
we will present our results either way, now that we know exactly how
to transform in both directions, between physical and dimensionless variables.

== A Meta-Recipe

*Alice*: Yes, that is what we decided.  How did you find the probability
distribution for the velocities?

*Bob*: I started with the distribution function for the energy of the
particles:

:equation:
\label{distributionfunction}
f(\br,\bv)d\br\bv \, = \, f(E(r,v)\,4\pi r^2 dr\,4\pi v^2dv \, = \,
       {384\sqrt(2)\over7\pi m}(-E)^{7/2}r^2v^2dr\,dv

*Alice*: Where did you get this expression from?

*Bob*: It's just what it is, for Plummer's model.  I found it in a useful
table in <i>The Gravitational Million-Body Problem</i>, by Douglas Heggie
and Piet Hut [Cambridge University Press, 2003].  It is table xxx on p. xxx,
a whole page full of useful properties of Plummer's model.  By the
way, <tex>$m$</tex> here is the mass of a single star, assuming that all
stars have equal mass.  If you multiply both sides of the equation with
<tex>$m$</tex>, you get <tex>$fm$</tex>, the mass density of stars in phase
space.

*Alice*: It is a power law in energy, and it looks like a polytrope.
Ah, yes, now I remember: Plummer's model is nothing else but a polytrope
of index five.  Polytropes are defined in general for index <tex>$n$</tex>
through a distribution function:

:equation:
f(E) \, \propto \, \left(|E|\right)^{n-3/2}

for negative energy, in other words for bound particles, while
<tex>$f(E)=0$</tex> for <tex>$E\ge0$</tex>, otherwise you would get
the whole universe filled with escapers.

But it is not fair to use such remembered knowledge, or equations that
you pluck from a book.  Our whole approach is aimed at spelling out
everything, both to help us in our research, to see new aspects we
might otherwise have overlooked, and to help us in our teaching, to
make things crystal clear to the students.

*Bob*: How do you expect to get new insight into the fact that the distribution
function of Plummer's model is a seven-halfth power of the energy?  That fact
will not change, no matter how long you stare at it.

*Alice*: That's not the point.  Once we spell out in complete detail how
you derive and verify all aspects of one recipe for one star cluster model,
you can then follow that example approach to construct any other recipe for
any other cluster model.  In other words, we are using Plummer's model as
a case study in order to present a meta-recipe for constructing recipes for
constructing models for star clusters.

*Bob*: I had no idea we were doing something that fancy.  But whatever words
you want to hang on it, I cannot deny that it is a good thing to check things
for yourself, and most importantly, I'll have to explain at least some
of these things in class pretty soon, so okay, let's go through the derivation.

However, we would probably lose the thread of our argument if we would
go into deriving eq. (ref(distributionfunction)) right now.  Let's postpone
that a bit, and first see whether we can reconstruct what I have written
in my program.  As you can expect, here too I just followed the recipe from
Aarseth <i>et al.</i>.  Let us first see whether we can at least derive that
recipe, assuming the validity of eq. (ref(distributionfunction)).

*Alice*: Sounds like a good plan.

== Following the Recipe

*Bob*: Here is what I have understood, so far, of the recipe.  Given the
distribution function for the energy, you have to transform that into an
equation for the magnitude for the velocity.  What makes life simpler, is
the fact that we are comparing particles with different velocity choices at
a given point, so we know that their potential energies are all the same.

This means that the probability <tex>$g(v)$</tex> to have an absolute
value for the velocity <tex>$v = |\bv|$</tex> at radial position
<tex>$r = |\br|$</tex> is given by

:equation:
g(v)dv \, \propto \, \left(-E(r,v)\right)^{7/2}v^2dv

where the energy per unit mass <tex>$E(r,v) = \Phi(r) + \half v^2$</tex>
can be written in terms of the escape velocity <tex>$v_e$</tex> as
<tex>$E(r,v) = -v_e^2 + \half v^2$</tex>.  If we introduce the
variable

:equation:
q \, = \, {v \over v_e}

we can write <tex>$E(q) \propto q^2 - 1$</tex>, for a given fixed
<tex>$r$</tex>.  The distribution function for <tex>$v$</tex> then
becomes, in terms of <tex>$q$</tex>, proportional to the following function:

:equation:
g(q) \, = \, \left(1 - q^2\right)^{7/2}q^2

*Alice*: And the range of admissable _q_ values is <tex>$0\le q \le 1$</tex>.
This looks exactly like the problem we had for determining the radial
positions.  There we knew the density, <i>i.e.</i> the probability function
to find a particle at a given position.  By integrating the density we
obtained the cumulative mass function <tex>$m(r)$</tex>, and then we
inverted that to obtain <tex>$r(m)$</tex>.

So I guess the next step is to invert <tex>$g(q)$</tex>.  However, that
doesn't look so easy.

*Bob*: To say the least.  Therefore, for the velocities, they
choose a different approach.  If you plot the function
<tex>$g(q)$</tex>, then the height of that curve, for each
<tex>$q$</tex> value, gives you the relative probability that
<tex>$q$</tex> would lie in a region of small fixed width around that
value.

You can imagine that you can obtain a distribution of the required
weighting by throwing darts at that graph.  If you hit a point
somewhere above the graph, you throw a new dart, and you keep throwing
new darts until you hit a point below the graph, anywhere between
<tex>$q=0$</tex> and <tex>$q=1$</tex>.  If you follow that procedure,
you are automatically guaranteed that you score more hits at places
where the graph is higher, and exactly so in proportion to the height
of a graph.

== A Rejection Technique

*Alice*: That is a clever solution.  It is called a rejection technique.
Didn't John von Neumann first apply that?  You start by allowing more
solutions than the minimal set of correct ones, and then you weed out
the incorrect ones, by rejecting them.

*Bob*: Indeed.  And to make the procedure efficient, you don't want to
throw darts way above the graph, so you limit yourself to the maximum
value that the graph attains in the interval of interest, or perhaps
a slightly higher value.  The authors of the paper choose a value of 0.1.

*Alice*: Is that a safe value?  Let's check for ourselves.  The derivative
of <tex>$g(q)$</tex> is

:eqnarray:
{dg(q)\over dq} \, &=& \, 
  2q\left(1 - q^2\right)^{7/2} -7q^3\left(1 - q^2\right)^{5/2}      \nonumber\\
&=& \, q\left(2 - 9 q^2\right)\left(1 - q^2\right)^{5/2}

To find the extrema for <tex>$g(q)$</tex>, we set the derative to zero,
and solve for <tex>$0<q_x<1$</tex>:

:equation:
q_x^2 = 2/9 \Rightarrow g(q_x) \, = \, (2/9)(7/9)^{7/2} \approx 0.092

Indeed: 0.1 is a rather tight upper limit.

*Bob*: You can now see what I did when I wrote:

 :inccode: .mkplummer1.rb-8

*Alice*: Ah, yes.  So +x+ stands for <tex>$q$</tex> and +y+ stands for
<tex>$g(q)$</tex>.  You keep throwing darts until you find a +y+
value under the graph.  That gives you the corresponding +x+ value.
Since this value is equal to <tex>$q = v / v_e$</tex>, you have to
multiply +x+ with the escape velocity <tex>$v_e$</tex>, which we found
in eq. (ref(escapevelocity)) to be:

:equation:
v_e(r)  \, = \, \sqrt{2}\left(r^2 + 1\right)^{-1/4}

Okay, I understand the procedure now, and it looks correct.

== Distribution Function

*Bob*: The only thing left to do now is to derive the form of the
distribution function.

*Alice*: Yes.  Let us see how far we can get.  Clearly we need a little help
from our friends: here is the classic reference <i>Galactic Dynamics</i>,
by James Binney and Scott Tremaine [Princeton University Press, 1987].
I have found it to be a very useful book, whenever I had to look up some
properties of particular models in stellar dynamics.  It also has helped
me a number of times to refresh my memory about Jeans equations, the
tensor virial theorem and those sort of things.

*Bob*: I see that your browsing was rather uneven: there is a piece, about
one third along the way, which has a grey strip.  Let me open it up there.
Aha, chapter 4: Equilibria of Collisionless Systems.  How come those pages
are so well-used?  I thought you were mainly interested in collisionial
systems?

*Alice*: I didn't realize my copy of Binney and Tremaine betrayed my past
browsing so well.  You're right, that's the chapter I tend to consult most.
And precisely because we are interested in _collisional_ systems, we have
to find a way to start our simulations with a _collisionless_ system.

In other words, we run a numerical simulation of a collisional N-body
system in order to see the effects of two-body relaxation and all that.
But we need to have a well-defined starting point.  A formal way to define
this is to say that we ignore collisions from time <tex>$t=-\infty$</tex>
all the way to <tex>$t=0$</tex>.  Then, at <tex>$t=0$</tex>, we switch on
the effects of close encounters, and through our simulations we can see
in what way our star clusters then begin to deviate from the dynamical
equilibrium we started with.

I'm sure you don't like such a formal way of speaking about it.  But
you can't complain: you started asking philosophical questions about
well-read chapters in someone else's book!  So there.

*Bob*: Okay, let's go get our distribution function.  Does this book
tell us how to do that?

*Alice*: Yes, but let us first see how far we can get under our own steam.
Let us go back all the way to how we got started, namely with the
Plummer potential:

<tex>$$
\Phi(r) \, = \, -\,GM {1\over \,\left(r^2 + a^2\right)^{1/2}}
$$</tex>

Let us define <tex>$\tilde f(\br, \bv)$</tex> as the density of stars
in phase space.  This means that in a six-dimensional volume element
<tex>$d\br d\bv$</tex>, you will find a number of particles equal to
<tex>$\tilde f(\br, \bv)d\br d\bv$</tex>.  We will restrict
ourselves to equal-mass systems, where each star has a mass
<tex>$m$</tex>.  This means that the volume element <tex>$d\br d\bv$</tex>
contains on average an amount of mass equal to
<tex>$m\tilde f(\br, \bv)d\br d\bv$</tex>.

In general, <tex>$\tilde f$</tex> will be time dependent, <i>i.e.</i>
of the form <tex>$\tilde f(\br, \bv)$</tex>, but here we will restrict
ourselves to dynamical equilibrium situations, where <tex>$\tilde f$</tex>
is constant, at least on a dynamical time scale, of order the crossing
time, the time it will take for a typical particle to cross the system.

Each star moving with velocity <tex>$v$</tex> at radial position
<tex>$r$</tex> has a kinetic energy <tex>$\half m v^2$</tex> and a
potential energy <tex>$m\Phi(r)$</tex>.  It is most convenient to talk
about specific energies, namely the energy per unit mass, <tex>$E$</tex>,
which is given as:

:equation:
E = \Phi(r) + \half v^2

In the case of spherical symmetry and isotropy, we have
<tex>$\tilde f(\br, \bv) = f(E)$</tex>  where the energy
<tex>$E(\br, \bv)$</tex> is the sum of the kinetic and potential energy
per unit mass of the particles that reside in the volume element
<tex>$d\br d\bv$</tex>.

Now you see why I started with that funny tilde over the distribution
function: since we will be dealing primarily with <tex>$f(E)$</tex>,
I prefered to use the <tex>$f$</tex> notation for a distribution
function with an <tex>$E$</tex> dependence, leaving the <tex>$\tilde f$</tex>
notation for the dependence of Cartesian phase space coordinates.

*Bob*: I still would be happy to not use tildes at all, but we'll each
follow our own way.

*Alice*: There is one more thing we definitely have to stress here.
Even though we have switched to <tex>$E$</tex> dependence, we still
have to write <tex>$f(E)d\br d\bv$</tex>, in order to find the number
of stars in the volume element <tex>$d\br d\bv$</tex>.

This is something students always get confused about.  It is tempting
to write <tex>$f(E)dE$</tex> instead, but that would be wrong: the
element <tex>$dE$</tex> would include all particles with specific
energy between <tex>$E$</tex> and <tex>$E+dE$</tex>, globally in the
system, which will amount to far more mass than is present in the
local volume element <tex>$\br,\bv$</tex>.

*Bob*: Noted!

== The Density as a Projection

*Alice*: The most fundamental stage for a star system in stellar dynamics is
phase space.  Because our physical eyes see what happens in
configuration space, we think about the density as a rather basic function.
But really, what we call density is already a type of shadow: it is a
projection down to 3D from the distribution function in 6D.  

In a star cluster with spherical symmetry in configuration space, we
can show this projection effect as follows.  The mass density in stars
at a distance <tex>$r$</tex> from the center is

:equation:
\label{densitygeneral}
\rho(r) = \int_0^\infty f(E) \, d\bv

where the integral is over all of velocity space.  If in addition to
spherical symmetry, the distribution function is isotropic, we know
that there is no direction dependence in <tex>$\bv$</tex>.  In that
case only the magnitude <tex>$v=|\bv|$</tex> can come into play.
We can thus substitute:

<tex>$$
d\bv \, = \, 4\pi \, v^2 dv
$$</tex>

Also, we know that the distribution function has to be zero for positive
energies, since unbound particles will escape, and their density in an
(almost) infinite universe will become (almost) zero:

<tex>$$
f(E) \, = \, 0  \,\,\,\,\, (E \ge 0)
$$</tex>

For a given radial distance <tex>$r$</tex>, the escape velocity is given
by definition as the velocity for which a particle can just reach infinity,
which means that the total energy is zero for that particle, and therefore
also the energy per unit mass.  As you already showed right at the beginning
of our discussion, this implies:

<tex>$$
E = \Phi(r) + \half v_{esc}^2 = 0 \, \Rightarrow \, v_{esc}(r) = 
\left(-2\Phi(r)\right)^{1/2}
$$</tex>

This means that we can rewrite eq. (ref(densitygeneral)) as:

<tex>$$
\rho(r) = 4\pi \int_0^{v_{esc}} f(E)\, v^2dv
$$</tex>

Now

<tex>$$
E \,=\, \Phi + \half v^2 \,\,\,\Rightarrow\,\,\, v^2 = 2(E-\Phi)
\,\,\, \Rightarrow
$$</tex>

<tex>$$
v \,=\, 2^{1/2}(E-\Phi)^{1/2} \,\,\,\Rightarrow
$$</tex>

<tex>$$
dv \,=\, 2^{-1/2}(E-\Phi)^{-1/2}\,dE
$$</tex>

So we get for the density:

:eqnarray:
\rho(r) &=& 4\pi \int_0^{v_{esc}} f(E)\, v^2dv                \nonumber\\
&=&  4\pi \int_0^{v_{esc}} f(E)\, 2(E-\Phi)\,
                   2^{-1/2}(E-\Phi)^{-1/2}\,dE                \nonumber\\
&=& 2^{5/2}\pi\int_\Phi^0(E-\Phi)^{1/2}f(E)dE

Here both <tex>$\Phi$</tex> and <tex>$E$</tex> are negative.
It is easier to introduce quantities that are positive.  Following the
notation used by Binney and Tremaine, we can define:

:equation:
\left\{ \begin{array}{lcl}
\Psi\,&=&\,- \, \Phi\\
\phantom{1}&\phantom{1}&\phantom{1} \\
\E\,&=&\,- \, E
\end{array} \right.

We can write again explicitly how the density in configuration space
is a projection down from phase space, through an integration over
<tex>$\E$</tex>:

:equation:
\label{rhofromcurlyE}
\rho(r) \,=\, 2^{5/2}\pi\int_0^\Psi (\Psi-\E)^{1/2}f(\E)d\E

Strictly speaking I should have changed the symbol for <tex>$f$</tex>
again since the functional form is different: <tex>$\hat f(\E)=f(E)$</tex>.

*Bob*: I'm glad you didn't!

*Alice*: I decided to take my hat off for you, or at least _f_'s hat.
Even for me, too much formality gets bothersome.

== A Change of Variables

*Bob*: Looking over your shoulder, I see that your eq. (ref(rhofromcurlyE))
is exactly eq. (4-137) in Binney and Tremaine, if you pull out their
<tex>$\sqrt2$</tex> in front of the integral.

*Alice*: Yes.  Let us follow their next few steps, for our particular
Plummer case.  They remark that <tex>$\Psi$</tex> is a monotonic function
of <tex>$r$</tex>, and therefore it is possible to use <tex>$\Psi$</tex>
as the independent variable in the expression for <tex>$\rho$</tex>.

*Bob*: That makes sense.  If <tex>$\Psi(r)$</tex> would not be monotonic,
there would be some values <tex>$r_1$</tex> and <tex>$r_2$</tex> with
<tex>$r_1 \ne r_2$</tex> for which <tex>$\Psi(r_1)=\Psi(r_2)$</tex>.
In that case, if you would write <tex>$\rho(\Psi)$</tex>, you would not
know whether you would be refering to <tex>$\rho(r_1)$</tex> or
<tex>$\rho(r_2)$</tex>.

*Alice*: Right.  But here everything is monotonic: the density keeps dropping
off when you move away from the center, the radius keeps increasing,
and the potential keeps increasing as well, or equivalently,
<tex>$\Psi=-\Phi$</tex> keeps decreasing.

*Bob*: So how do we get from <tex>$\rho(r)$</tex> to <tex>$\rho(\Psi)$</tex>?
That can't be difficult.  We have 

<tex>$$
\rho(r)dr\,=\,\rho(\Psi)d\Psi \,\,\, \Rightarrow \,\,\,
\rho(\Psi) \,=\, {dr \over d\Psi}\rho(r)
$$</tex>

And the conversion factor is nothing less than the inverse of the
gravitational acceleration, which I derived early on, with the
prophetic remark that it would come in handy later on.

*Alice*: No.

*Bob*: No?

*Alice*: No, there is no conversion factor.  Think about what density
means, in this context.  The mass density <tex>$\rho(r)d\br$</tex> is the
amount of mass in stars within the volume <tex>$d\br$</tex>, and _not_
within the whole shell of the cluster between <tex>$r$</tex> and
<tex>$r+dr$</tex>.  The latter amount of mass would be
<tex>$4\pi r^2\rho dr$</tex>.

This is the same thing as what we had seen with the phase space
distribution function <tex>$f$</tex>: the physical meaning does not change,
where we write it as <tex>$f(\br,\bv)$</tex> or as <tex>$f(E)$</tex>.
In both cases we multiply <tex>$f$</tex> with the volume element
<tex>$d\br d\bv$</tex>.

*Bob*: You're right!  How tricky.  Or how stupid of me.

*Alice*: Let's call it tricky.  Believe me, I've made these mistakes
often enough myself.

== Deprojecting the Density

*Bob*: So eq. (ref(rhofromcurlyE)) becomes simply:

<tex>$$
\rho(\Psi) \,=\, 2^{5/2} \pi\int_0^\Psi (\Psi-\E)^{1/2}f(\E)d\E
$$</tex>

*Alice*: Indeed.  If we divide both sides of the equation by
<tex>$\sqrt8\pi$</tex>, we get Binney and Tremaine's eq. (4-138):

:equation:
\label{rhofrompsi}
{1\over\sqrt8\pi}\,\rho(\Psi) \,=\, 2 \int_0^\Psi (\Psi-\E)^{1/2}f(\E)d\E

*Bob*: Why would they divide by that funny factor?

*Alice*: In order to get a simple looking right-hand side in their next
equation (4-139), which they obtain by differentiating eq. (ref(rhofrompsi))
with respect to <tex>$\Psi$</tex>:

:equation:
\label{drhodpsifrompsi}
{1\over\sqrt8\pi}\,{d\rho\over d\Psi} \,=\,
\int_0^\Psi f(\E)\,{d\E\over\sqrt{\Psi-\E}}

*Bob*: Ah, and here they do their deprojecting: they claim that this equation
can be inverted.  Eq. (ref(drhodpsifrompsi)) gives you <tex>$d\rho/d\Psi$</tex>,
when you give it <tex>$f(\E)$</tex>.  What we want is to obtain
<tex>$f(\E)$</tex>, and we can certainly figure out how to compute
<tex>$d\rho/d\Psi$</tex>.  I see the logic now.

*Alice*: Yes, and they give the solution of inverting Eq.
(ref(drhodpsifrompsi)), as their eq. (4-140a):

:equation:
\label{fcurlyEa}
f(\E)\,=\,{1\over\sqrt8\pi^2}\,{d\over d\E}
\int_0^\E {d\rho\over d\Psi} {d\Psi\over\sqrt{\E-\Psi}}

and in alternative form as their (4-140b):

:equation:
\label{fcurlyEb}
f(\E)\,=\,{1\over\sqrt8\pi^2}\,\left[
\int_0^\E {d^2\rho\over d\Psi^2} {d\Psi\over\sqrt{\E-\Psi}}
+{1\over\sqrt\E}\left({d\rho\over d\Psi}\right)_{\Psi=0}\right]

*Bob*: Great!  That will make it finally possible for us to determine the
distribution function: with a little luck we can solve that integral.

*Alice*: Not so quick: how do we know that they inverted eq.
(ref(drhodpsifrompsi)) correctly?

*Bob*: There you go again!  You don't take anything on authority, won't you?

*Alice*: Well, no, preferably not.  The whole reason to get embarked on this
exercise is to proof things for ourselves, from scratch.  I won't call
it starting from scratch if we simply accept their inversion.  We
might as well have accepted their expression for the distribution function
in the first place.

*Bob*: Which would have been fine with me, to be honest.  But okay, we got
started, we may as well finish.  Though I really begin to doubt that our
students will have the stomach to go through al this.

*Alice*: The best ones will.  And those are the ones we'd like to stay in
touch with.  So this may a good way to find good students, to see who are
interesting in figuring all this out to the bottom.

*Bob*: I'm not so sure.  You might just get formalistic and pedantic students.
I prefer to work with students who get codes to run and who get
results with codes, rather than students who can solve this inversion equation
-- what do they call it? -- an Abel integral equation.

*Alice*: The _very_ best students will be able to do both.

*Bob*: As able as Abel.

*Alice*: Such a silly pun would work better if you had an accent.  Come,
let's take our usual approach: first let's assume that
eqs. (ref(fcurlyEa,fcurlyEb)) are indeed correct, and let us check whether
we can then derive the correct distribution function for Plummer's model.
Having done that, we will come back, and check for ourselves whether we
can prove this Abel integral equation inversion.

*Bob*: Something tells me it will be a long day.

==








== xxx


*Alice*: By insisting that we have the same amount of mass in a thin shell,
whether we label the shell through its <tex>$r$</tex> or its <tex>$\Psi$</tex>
coordinate:

== xxx

== xxx

If we start with:

:equation:
f(x) = \int_0^x {g(t)dt \over (x-t)^\alpha}
\quad\quad\quad\quad  0<\alpha<1

then the inverse solution is:

:equation:
g(t) = {\sin \pi \alpha \over \pi}\,{d\over dt} \,
       \int_0^t {f(x)dx \over (t-x)^{1-\alpha}}

To prove this, let us call the right hand side <tex>$h(t)$</tex>,
and work it out, to see whether we really get back <tex>$g(t)$</tex>
in the end.

:eqnarray:
h(t) &=& {\sin \pi \alpha \over \pi}\,{d\over dt} \,
   \int_0^t {dx \over (t-x)^{1-\alpha}}\,
   \int_0^x {g(t')dt' \over (x-t')^\alpha}                        \nonumber\\
&=& {\sin \pi \alpha \over \pi}\,{d\over dt} \,
\int_0^t dx \int_0^x dt'{g(t')\over (t-x)^{1-\alpha}(x-t')^\alpha}  \nonumber\\
&=& {\sin \pi \alpha \over \pi}\,{d\over dt} \,
\int_0^tdt'\int_{t'}^t dx{g(t')\over(t-x)^{1-\alpha}(x-t')^\alpha}  \nonumber\\
&=& {\sin \pi \alpha \over \pi}\,{d\over dt} \,
\int_0^tdt'g(t')\int_{t'}^t {dx\over(t-x)^{1-\alpha}(x-t')^\alpha}

Let us look at the  second integral:

:equation:
I_2 = \int_{t'}^t {dx\over(t-x)^{1-\alpha}(x-t')^\alpha}

We can rewrite this by introducing

:equation:
y = x - t' \, \,\,\Rightarrow\,\, \, x = y + t'  \, \,\,\Rightarrow

:equation:
I_2 = \int_0^{t-t'} {dy\over(t-t'-y)^{1-\alpha}y^\alpha}

We can then rewrite this in terms of

:equation:
z = {y \over t -t'} \, \,\,\Rightarrow\,\, \, y = (t-t')z\, \,\,\Rightarrow

:equation:
t-t'-y = (t-t')(1-z)\, \,\,\Rightarrow

:eqnarray:
I_2 &=& \int_0^1 {(t-t')dz\over(t-t')^{1-\alpha}(1-z)^{1-\alpha}
(t-t')^\alpha z^\alpha}                       \nonumber\\
&=& \int_0^1 {dz\over(1-z)^{1-\alpha} z^\alpha}     \nonumber\\
&=& {\pi \over \sin \pi \alpha}

and therefore

:equation:
h(t) = {d\over dt} \int_0^t g(t')dt' = g(t)

We can work out the expression for <tex>$g(t)$</tex> by taking the
time derivative of the integral, by substituting

:equation:
w = t - x \, \,\,\Rightarrow\,\, \, x = t -w  \, \,\,\Rightarrow

x

x

x

:eqnarray:
g(t) &=& {\sin \pi \alpha \over \pi}\,{d\over dt} \,
       \int_0^t {f(x)dx \over (t-x)^{1-\alpha}}                    \nonumber\\
&=& {\sin \pi \alpha \over \pi}\,{d\over dt} \,
\int_0^t {f(t-w)dw \over w^{1-\alpha}}                             \nonumber\\
&=& {\sin \pi \alpha \over \pi}\, \left[
\left.{f(t-w) \over w^{1-\alpha}}\right|_{w=t}+\int_0^t \left({d\over dt}f(t-w)\right)
{dw \over w^{1-\alpha}} \right]                                    \nonumber\\
&=& {\sin \pi \alpha \over \pi}\, \left[
{f(0)\over t^{1-\alpha}} + \int_0^t \left({d\over dx}f(x)\right)
{dx \over (t-x)^{1-\alpha}} \right]                                \nonumber\\
&=& {\sin \pi \alpha \over \pi}\, \left[
{f(0)\over t^{1-\alpha}} + \int_0^t {df\over dx}
{dx \over (t-x)^{1-\alpha}} \right]                               

