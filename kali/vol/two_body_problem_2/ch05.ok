= A Complete Derivation

== The One-Dimensional Case

*Bob*: You have a very determined look in your eyes.

*Alice*: Yes, I feel determined.  I want to know for sure that the
integrator listed in Abramowitz and Stegun is fourth-order, and I
want to prove it.

In the form given in their book, they are dealing with a force that
has the most general form: it depends on position, velocity, and time.
In our notation, their differential equation would read

<tex>$$
\frac{d^2}{dt^2}{\bf r} \, = \, {\bf f}(\br, \bv, t)\, =
 \, {\bf f}(\br(t), \bv(t), t)
$$</tex>

*Bob*: Fortunately, in our case, we only have to deal with a dependence
on position, not on velocity or time.

*Alice*: That is true, but I would like to prove their original formula.

*Bob*: I don't think that is necessary.  We don't have any magnetic force,
or anything else that shows forces that depend on velocity.

*Alice*: Yes, but now that we've spotted what seems like a mistake, I don't
want to be too facile.  Okay, let's make a compromise.  I'll drop the
velocity dependence; hopefully our derivation, if successful, should
point the way to how you could include that extra dependence.  But I would
like to keep the time dependence, just to see for myself how that mixes
with the space dependence.

And if you want a motivation: imagine that our star cluster is suspended
within the background tidal field of the galaxy.  Such a field can be treated
as a slowly varying, time-dependent perturbation, exactly of this form:

<tex>$$
\frac{d^2}{dt^2}{\bf r} \, = \, {\bf f}(\br, t)\, = \, {\bf f}(\br(t), t)
$$</tex>

*Bob*: Fair enough.  How shall we start?

*Alice*: If we _really_ wanted to be formally correct, we could track
the four-dimensional variations in space and time, taking into account
the three-dimensional spatial gradient of the interaction terms.

*Bob*: But from the way you just said that, I figure that that would be
too much, even for you.  I'm glad to hear that even you have your limits
in wanting to check things analytically!

*Alice*: I sure have.  And also, I don't think such a formal treatment
would be necessary.  If we can prove things in one dimension, I'm convinced
that we could rewrite our prove to three dimensions without too many problems,
albeit with a lot more manipulations.

== A Change in Notation

*Bob*: You won't hear me arguing against simplification!
But I propose that we use a somewhat more intuitive notation than what
Abramowitz and Stegun gave us.  To use _x_ to indicate time and _y_ to
indicate position is confusing, to say the least.  Let us look at the
text again:

link:rk4.gif

How about the following translation, where the position _r_, velocity _v_,
and _a_ are all scalar quantities, since we are considering an effectively
one-dimensional problem:

 :eqnarray:
x \,\,\,\,\,\,\,\,\,\,\,\,\, &\rightarrow& \,\,\,\,\,\,\,\,\,t     \nonumber\\
y \,\,\,\,\,\,\,\,\,\,\,\,\, &\rightarrow& \,\,\,\,\,\,\,\,\,r     \nonumber\\
f(x,y) \,\,\,\,\, &\rightarrow& \,\,\,f(r,t)                       \nonumber\\
y' = {dy \over dx}  \,\,\, &\rightarrow& \,\,\,
\dot r = {dr \over dt} = v                                         \nonumber\\
y'' = {d^2y \over dx^2}  &\rightarrow& \,\,\,\,
\ddot r = {d^2r \over dt^2} = {dv\over dt} = a                     \nonumber\\
h \,\,\,\,\,\,\,\,\,\,\,\,\, &\rightarrow&  \Delta t = \tau        \nonumber\\
k_i \,\,\,\,\,\,\,\,\,\,\, &\rightarrow& \,\,\,\,\,\,\, k_i \tau   \nonumber\\
n \,\,\,\,\,\,\,\,\,\,\,\, &\rightarrow& \,\,\,\,\,\,\,\,\,\, 0    \nonumber

The equations, as proposed by Abramowitz and Stegun, then become:

 :eqnarray:
\label{base}
\ddot r\,&=&\,f(r, t)                                              \nonumber\\
&&                                                                 \nonumber\\
r_1 \,&=&\, r_0 + 
\left(v_0+\one{6}\left(k_1+2k_2\right)\tau\right)\tau              \nonumber\\
v_1 \,&=&\, v_0 + 
\one{6}\left(k_1+4k_2+k_3\right)\tau                               \nonumber\\
&&                                                                 \nonumber\\
k_1\,&=&\,f(r_0, 0)                                                \nonumber\\
k_2\,&=&\,f(r_0 + \hhalf v_0\tau + \one{8}k_1\tau^2, \hhalf\tau)   \nonumber\\
k_3\,&=&\,f(r_0 + v_0\tau + \hhalf k_2\tau^2, \tau)

== The Auxiliary _k_ Variables

*Alice*: Yes, I find that notation much more congenial, and I'm glad to see
the explicitly space and time dependence now in the definitions of the three
_k_ variables.

Let us work out what these _k_ variables actually look like, when we
substitute the orbit dependence in the right hand side.  What we see
there is an interesting interplay between a physical interaction term
_f_ that has as an argument the mathematical orbit <tex>$r(t)$</tex>.

Since you have introduced <tex>$\tau$</tex> as the increment in time,
starting from time zero, let me introduce the variable <tex>$\rho_1$</tex>
to indicate the increment in space, the increment in position at the time
that we evaluate <tex>$k_2$</tex>

:equation:
\label{rho1}
\rho_1 \,=\, \hhalf v_0\tau + \one{8}k_1\tau^2

For consistency, let me also use

:equation:
\tau_1 \,=\, \hhalf \tau

for the increment in time, when we evaluate <tex>$k_2$</tex>.
Let me also abbreviate <tex>$f_0 = f(r_0,0)$</tex>.

What I propose to do now is to expand <tex>$k_1$</tex> in a double Taylor
series, simultaneously in space and in time.  This will hopefully repair
the oversight we made before, when we only considered a time expansion.

*Bob*: How do you do a two-dimensional Taylor series expansion?

*Alice*: Whenever you have mixed derivatives, such as

:equation:
{\partial^3 f \over \partial r^2\partial t} \,=\, f_{rrt}

you multiply the coefficients you would give to each dimension separately;
in this case you would get a factor of one half, because of the second
partial derivative with respect to space; the single time derivative
just gives a factor one.  You can easily check how this work when you
look at a two-dimensional landscape, and you imagine how you pick up
difference in height by moving North-South and East-West.  Anyway, here
is the concrete example, for our case, where I use the abbreviation that
I introduced above, with partial differentiation indicated with subscripts:

 :eqnarray:
k_2 \,=\, f_0 &+&\, f_r\rho_1 + f_t\tau_1                           \nonumber\\
&+&\,\hhalf f_{rr}\rho_1^2+f_{rt}\rho_1\tau_1+\hhalf f_{tt}\tau_1^2 \nonumber\\
&+&\,\one{6}f_{rrr}\rho_1^3+\hhalf f_{rrt}\rho_1^2\tau_1
     +\hhalf f_{rtt}\rho_1\tau_1^2 + \one{6}f_{ttt}\tau_1^3 + O(\tau_1^4)

where I have grouped the different orders of expansion in terms of small
variables on different lines.  Using the fact that
<tex>$\tau_1 = \hhalf \tau$</tex>, we can also write this as

 :eqnarray:
\label{k2}
k_2 \,=\, f_0 &+&\, f_r\rho_1 + \hhalf f_t\tau                    \nonumber\\
&+&\,\hhalf f_{rr}\rho_1^2+
\hhalf f_{rt}\rho_1\tau+\one{8}f_{tt}\tau^2                       \nonumber\\
&+&\,\one{6}f_{rrr}\rho_1^3+\one{4}f_{rrt}\rho_1^2\tau
     +\one{8}f_{rtt}\rho_1\tau^2 + \one{48}f_{ttt}\tau^3 + O(\tau_1^4)

Similarly, we can introduce

:equation:
\label{rho2}
\rho_2 \,=\, v_0\tau + \hhalf k_2\tau^2

as the spatial offset in the argument for the interaction term for the
calculation of <tex>$k_3$</tex>, for which we get:

 :eqnarray:
\label{k3}
k_3 \,=\, f_0 &+&\, f_r\rho_2 + f_t\tau                           \nonumber\\
&+&\,\hhalf f_{rr}\rho_2^2+f_{rt}\rho_2\tau+\hhalf f_{tt}\tau^2   \nonumber\\
&+&\,\one{6}f_{rrr}\rho_2^3+\hhalf f_{rrt}\rho_2^2\tau
     +\hhalf f_{rtt}\rho_2\tau^2 + \one{6}f_{ttt}\tau^3 + O(\tau_1^4)

Now the next step is to substitute the definitions of the
<tex>$\rho_i$</tex> variables in the <tex>$k_{i+1}$</tex> equations,
and to keep terms up to the third order in <tex>$\tau$</tex>.

== The Next Position

*Bob*: That's going to be quite messy.

*Alice*: Well, yes, but we have no choice.  Let me start with
the substitution indicated in Eq. (ref(rho1)), where we may as well
replace <tex>$k_1$</tex> by its value <tex>$f_0$</tex>:

:equation:
\label{rho1s}
\rho_1 \,=\, \hhalf v_0\tau + \one{8}f_0\tau^2

which is just its Taylor series expansion in time, up to second order.
Using this substitution in Eq. (ref(k2)), we get:

 :eqnarray:
\label{k2s}
k_2 \,=\, f_0 &+&\, \left\{
\hhalf v_0 f_r + \hhalf f_t
\right\} \tau                                           \nonumber\\
&+&\, \left\{
\one{8}f_0f_r + \one{8}v_0^2f_{rr}
+\one{4}v_0f_{rt}+\one{8}f_{tt}
\right\} \tau^2                                           \nonumber\\
&+&\, \left\{
\one{16}v_0f_0f_{rr} + \one{16}f_0f_{rt} + \right.                \nonumber\\
&& \ \ \left. \one{48}v_0^3f_{rrr}
+ \one{16}v_0^2f_{rrt}+ \one{16}v_0f_{rtt}+ \one{48}f_{ttt}
\right\} \tau^3 + O(\tau_1^4)

Similarly, we have to take Eq. (ref(k3)), where we have to use the
substitution indicated in Eq. (ref(rho2)), which we can rewrite, using
the expression for <tex>$k_2$</tex> that we just derived, up till third
order in <tex>$\tau$</tex>, as;

:equation:
\label{rho2s}
\rho_2 \,=\, v_0\tau + \hhalf f_0\tau^2 + \left\{
\one{4} v_0 f_r + \one{4} f_t \right\} \tau^3 + O(\tau_1^4)

*Bob*: And now I see concretely what you meant, when you said that our
previous attempt at proving the fourth-order nature of the Runge-Kutta
integrator neglected the treatment of the right-hand side of the
differential equation, of the physical interaction terms.  They would
not have showed up if we were trying to prove the correctness of a
second-order Runge-Kutta scheme.  It is only at third order in
<tex>$\tau$</tex> that they appear, in the form of partial derivatives
of the interaction term.

*Alice*: Yes, that must be right.  Okay, ready for a next round of
substitution?  Eq. (ref(k3)) then becomes:

 :eqnarray:
\label{k3s}
k_3 \,=\, f_0 &+&\, \left\{
v_0 f_r + f_t
\right\} \tau                                           \nonumber\\
&+&\, \left\{
\hhalf f_0f_r + \hhalf v_0^2f_{rr}
+v_0f_{rt}+\hhalf f_{tt}
\right\} \tau^2                                           \nonumber\\
&+&\, \Big\{
\one{4}v_0\left(f_r\right)^2 + \one{4}f_rf_t +
\hhalf v_0f_0f_{rr} + \hhalf f_0f_{rt} +                \nonumber\\
&& \ \ \one{6}v_0^3f_{rrr}
+ \hhalf v_0^2f_{rrt}+ \hhalf v_0f_{rtt}+ \one{6}f_{ttt}
\Big\} \tau^3 + O(\tau_1^4)

We are now in a position to evaluate the solution that our Runge-Kutta
scheme gives us for the value of the position and velocity in the next
time step, as given in Eq. (ref(base)), since there we can now
substitute <tex>$k_2$</tex> and <tex>$k_3$</tex> in the expressions for
<tex>$r_1$</tex> and <tex>$v_1$</tex>.

Starting with the position, we find:

 :eqnarray:
\label{r1}
r_1 \,=\, r_0 &+&\, v_0\tau + \hhalf f_0\tau^2 + \one{6}\left\{
v_0 f_r + f_t \right\} \tau^3                                      \nonumber\\
&+&\, \one{24}\left\{
f_0f_r + v_0^2f_{rr} +2v_0f_{rt}+ f_{tt}
\right\} \tau^4 + O(\tau_1^5)

== The Next Velocity

*Bob*: And the last term in curly brackets, in the first line of the
last equation must be the jerk.  I begin to see some structure in these
expressions.

*Alice*: Yes.  But let me get the velocity as well, and then we can take
stock of the whole situation.  Let's see.  To solve
for <tex>$v_1$</tex> in Eq. (ref(base)), we need to use the following
combination of <tex>$k_i$</tex> values:

 :eqnarray:
4k_2+k_3 \,=\, 5f_0 &+&\, \left\{
3v_0 f_r + 3f_t
\right\} \tau                                           \nonumber\\
&+&\, \left\{
f_0f_r + v_0^2f_{rr}
+2v_0f_{rt}+ f_{tt}
\right\} \tau^2                                           \nonumber\\
&+&\, \Big\{
\one{4}v_0\left(f_r\right)^2 + \one{4}f_rf_t +
{\textstyle\frac{3}{4}} v_0f_0f_{rr} + {\textstyle\frac{3}{4}} f_0f_{rt}
+   \nonumber\\
&& \ \ \,\one{4}v_0^3f_{rrr}
+ {\textstyle\frac{3}{4}} v_0^2f_{rrt}+ {\textstyle\frac{3}{4}} v_0f_{rtt}
+ \one{4}f_{ttt} \Big\} \tau^3 + O(\tau_1^4)                        \nonumber

Plugging this back into the expression for <tex>$v_1$</tex> in Eq. (ref(base)),
we get:

 :eqnarray:
\label{v1}
v_1 \,=\, v_0 &+&\, f_0\tau + \hhalf \left\{
v_0 f_r + f_t \right\} \tau^2                                      \nonumber\\
&+&\, \one{6}\left\{
f_0f_r + v_0^2f_{rr} +2v_0f_{rt}+ f_{tt}
\right\} \tau^3                                                    \nonumber\\
&+&\, \one{24}\Big\{
v_0\left(f_r\right)^2 + f_rf_t +
3v_0f_0f_{rr} + 3f_0f_{rt} \nonumber\\
&& \ \ \ \ \ \ v_0^3f_{rrr}
+ 3v_0^2f_{rrt}+ 3v_0f_{rtt}
+ f_{ttt} \Big\} \tau^4 + O(\tau_1^5)

*Bob*: Quite an expression.  Well done!

*Alice*: And an expression that makes sense: we see that indeed the
velocity, Eq. (ref(v1)),
is the time derivative of the position, Eq. (ref(r1)), up to the order given
there.  So everything is consistent.

*Bob*: But this is really astonishing.  Why would this last expression
be also fourth-order accurate?  Even if the fourth-order term had been
totally wrong, the velocity in Eq. (ref(v1)) would still have been the
time derivative of Eq. (ref(r1)), up to the accuracy to which that
expression holds.  We had no right to expect more!

*Alice*: It is quite remarkable.  But I presume that whoever derived this
equation know what he or she was doing, in choosing the right coefficients!
Well, most likely a he, I'm afraid.  I think this equation goes back
to a time in which there were even fewer women in mathematics than
there are now.

*Bob*: Now that you mention it, I do wonder who derived this marvel.  It
still surprises me, that you really can get fourth-order accurate, while
using only three function evaluations.

*Alice*: Yes, that is neat.  It must have something to do with the fact
that we are dealing with a second-order equation.  Even though the force
term for the acceleration, as the derivative of the velocity, can be
arbitrarily complex, the derivative of the position couldn't be simpler:
it is always just the velocity.

*Bob*: I bet you're right.  After all, Abramowitz and Stegun list this
algorithm specifically under the heading of second-order differential
equations.  They must have had a reason.

*Alice*: I'd like to trace the history, now that we've come this far.
But first, let's fiish our derivation.

*Bob*: I thought we had just done that?  We've shown that both position
and velocity are fourth-order accurate.

*Alice*: Well, yes, but by now I've grown rather suspicious.  We have
already seen instances where a derivation looked perfect on paper, but
then it turned out to be only formally valid, and not in practice.

*Bob*: Hard to argue with that!

== Everything Falls into Place

*Alice*: What we have done now is to compute the predicted positions and
velocities after one time step, and we have checked that if we vary the
length of the time step, we indeed find that the velocity changes like the
time derivative of the position.  The question is: are they really solutions
of the equations of motion that we started out with, up to the fourth order
in the time step size?

Let us go back to the equations of motion, in the form we gave them
just after we translated the expressions from Abramowitz and Stegun:

:equation:
a(t) = \ddot r(t)\,=\,f(r(t), t)

For time zero this gives:

:equation:
a_0 = f_0

We can differentiate the equation for <tex>$a(t)$</tex>, to find the jerk:

:equation:
j(t) \,=\, {d\over dt}f(r(t), t) \,=\, f_r {dr\over dt} + f_t 
\,=\, v(t)f_r + f_t

which gives us at the starting time:

:equation:
j_0 \,=\, v_0 f_r + f_t

When we differentiate the equation for <tex>$j(t)$</tex>, we find the snap:

:equation:
s(t) \,=\, {d\over dt}j(t) \,=\, a(t)f_r + \left(v(t)\right)^2f_{rr}
+ 2v(t)f_{rt} + f_{tt}

or at time <tex>$t=0$</tex> :

:equation:
s_0 \,=\, f_0f_r + v_0^2f_{rr} +2v_0f_{rt}+ f_{tt}

Next, with one more differentiation of the equation for <tex>$s(t)$</tex>,
we can derive the crackle:

 :eqnarray:
c(t) \,=\, {d\over dt}s(t) &=& j(t)f_r \ + \  kira4now3v(t)a(t)f_{rr} \ +
 \  3a(t)f_{rt}                                                   \nonumber\\
&+& \left(v(t)\right)^3f_{rrr} + 3\left(v(t)\right)^2f_{rrt}
+ 3v(r)f_{rtt} + f_{ttt}

Using the expression for the jerk, that we found above, we find for
time zero:

:equation:
c_0 \,=\, v_0\left(f_r\right)^2 + f_rf_t + 3v_0f_0f_{rr} + 3f_0f_{rt} 
+ v_0^3f_{rrr} + 3v_0^2f_{rrt}+ 3v_0f_{rtt} + f_{ttt} \ \ \ \ 

*Bob*: I see, now the clouds are clearing, and the sun is shining through!
When you substitute these values back into the equation for the new position,
Eq. (ref(r1)), we find:

:equation:
r_1 \,=\, r_0 +\, v_0\tau + \hhalf a_0\tau^2 + \one{6}j_0\tau^3
+ \one{24}s_0\tau^4

and similarly, for Eq. (ref(v1)), we find:

:equation:
v_1 \,=\, v_0 +\, a_0\tau + \hhalf j_0\tau^2 + \one{6}s_0\tau^3
+ \one{24}c_0\tau^4

Both are nothing else but the Taylor series for their orbits.

*Alice*: And now we have derived them in a fully space-time dependent
way.

*Bob*: Congratulations!  I'm certainly happy with this result.  But I must
admit, I do wonder whether this conclusion would satisfy a
mathematician working in numerical analysis.

*Alice*: Probably not.  Actually, I hope not.  I'm glad some people are more
careful than we are, to make sure that our algorithms really do what
we hope they do.  At the same time, I'm sure enough now that we have a
reasonable understanding of what is going on, and I'm ready to move on.

*Bob*: I certainly wouldn't ask my students to delve deeper into this matter
than we have been doing so far.  At some point they will have to get results
from their simulations.

*Alice*: At the same time, unless they have a fair amount of understanding
of the algorithms that they rely on, it will be easy for them to use those
algorithms in the wrong way, or in circumstances where their use is
not appropriate.  But I agree, too much of a good thing may simply be
too much.  Still, if a student would come to be with a burning curiosity
to find out more about what is _really_ going on with these
higher-order integration methods, I would encourage that him or her to
get deeper into the matter, through books or papers in numerical analysis.

*Bob*: I don't think that is too likely to happen, but if such students
appear on my doorsteps, I'm happy to send them to you!

*Alice*: And I would be happy too, since I might well learn from them.
I've the feeling that we've only scratched the surface so far.

== Debugging All and Everything?

*Bob*: Yes, and our scratching had some hits and some misses.
If we are ever going to write up all this exploring we are doing now,
for the benefit of our students, we have to decide what to tell them, and
what to sweep under the rug.

*Alice*: I suggest we tell them everything, the whole story of how and where
we went wrong.  If we're going to clean up everything, we will wind up with
a text book, and text books there are already enough of in this world.

*Bob*: I wasn't thinking about any formal publication, good grief, that would
be far too much work!  I was only thinking about handing out notes to the
students in my class.  And I must say, I like the idea of showing them
a look into our kitchen.  I think it will be much more interesting for the
students, and after all, if you and I are making these kinds of mistakes,
we can't very well pretend that the students would be above making those
types of mistakes, can we?

*Alice*: Precisely.  They may learn from our example, in two ways: they will
realize that everyone makes mistakes all the time, and that every mistake
is a good opportunity to learn more about the situation at hand.  Debugging
is not only something you have to do with computer programs.  When doing
analytical work, you also have to debug both the equations you write down,
in which you will make mistake, as well as the understanding you bring to
the writing down in the first place.

You might even want to go so far as to say that life is one large debugging
process.  I heard Gerald Sussman saying something like that, some day.  He
was in a talkative mood, as usual.  Somebody asked him what he meant
when he said that he considers himself to be a philosophical engineer, and
that was his answer.

*Bob*: I doubt that I will have to debug my beer, I sure hope not!  But as
far as codes and math and physics, sure, I'd buy that.  So yes, let us
present the students our temporary failures on the level of math as well
as on the level of coding.  I'm all for it!

*Alice*: So, what's next?  A sixth order integrator?

*Bob*: I hadn't thought about that, but given the speed-up we got with
going from <tt>rk2</tt> to <tt>rk4</tt>, it would be interesting to
see whether this happy process will continue when we would go to a
<tt>rk6</tt> method, for a sixth-order Runge-Kutta method.  The
problem is that Abramowitz and Stegun don't go beyond fourth order
methods.  Hmm.  I'll look around, and see what I can find.

*Alice*: Meanwhile, I will try and track down the history of our
mysterious formulae.
