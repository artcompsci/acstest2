Date: Sun Jan 09 12:40:03 JST 2005

From: Piet Hut <piet@ias.edu>

Hi Jun:

Here is a brief summary of the many things we have talked about this week.
If you think I have missed anything important, please let me know.

* Monday, 2004-1-3

-- a basic code

We talked about the idea of creating a general basic code that will
have the basic structures that we think may be needed for future,
more specialized codes.  It may contain one or more trees; it would
be nice if we could find a four-dimensional generalization of both
the 3-D spatial oct tree and temporal block steps.

The simplest code that is guaranteed to work, for small enough time
steps, is a shared but variable time step code.  This will be terribly
inefficient.

The question is, how to write a more efficient code, with a reasonable
attempt to optimize the trade-off between computer time and accuracy.
It doesn't have to be the optimal choice, but it would be nice if it
would not be orders of magnitude slower than the optimal choice.  The
most important criterion, however, would be that it would not crash,
and also that it would not slow down to a crawl that does not converge.

Is it possible to proceed in steps?  Can we define the next step,
better than shared time steps, but not yet optimal?

The problem is that, as soon as you introduce individual time steps,
all kind of problems pop up simultaneously.  For example, particles
that suddenly start moving very fast, as a result of a three-body
encounter for example, may break simple time step criteria: they may
need special treatments.  Also, if you want to attain high accuracy,
it may be necessary for every star to resolve the high-frequency noise
generated by a single close binary -- unless you can come up with a
clever averaging scheme and perturber scheme.

We can reformulate the question above as follows:

There are two ways to improve an N-body code beyond shared timesteps:

Bottom-up: introduce individual time steps, and try to deal in an
approximate way with the many problems that pop up, even though some
of those are probably (quite?) incorrect.

Top-down: try to design a basic code that has all (or at least many)
of the tools that may be needed eventually, and try to get that one
to work in a way that at least doesn't crash.  Once such a code works,
you can try to optimize it step-by-step, to get better performance.

Traditionally, the bottom-up path has been followed.  What shall we do
in ACS?  Bottom-up?  Top-down?  A combination between the two?

-- some ideas

Here are some ideas that came up:

A basic code could be built from a mutual-nearest-neighbor tree, like
what Jernigan and Porter used; it has the advantage over an oct tree
that it is coordinate independent.  The basic type of interaction
could be pairwise force calculations -- but probably in a tree-grouped
way, since the force from a tight binary on far away particles behaves
much better than the forces of each of the components, which largely
cancel each other, since only the quadrupoles and higher survive.
Perhaps a natural idea would be to use a (log N)-fold Ahmad-Cohen code,
where each node-node force calculation is treated as a separate
Ahmad-Cohen region.

We could use arbitrary precision; would that make it possible to design
a code without using offset regularization?  Perhaps not; accumulation
of truncation errors might still give orbit integration properties that
would be much worth than if they were done for relative distances in a
binary tree.  Binary trees thus introduce a form of stabilization:
the center of mass of each binary is guaranteed to be unaffected by
truncation errors on the level of the relative motion of two nodes.

-- provably correct

We cannot aim at writing a provably correct code, much as we would like.
Already on a physical level, there are singularities that cannot be
treated analytically, such as simultaneous 3-body calculations, and
also singularities that are only partly understood, such as the 5-body
singular behaviors in finite time in which at least one particle
reaches infinite velocity.

Can we settle for a statistically correct code?  But statistical in
what sense?  For a homogenous distribution of particles in thermal
equilibrium?  Or for a spherically concentrated distribution?  Should
it also contain primordial binaries, triples, etc?

One option to get more of a handle on provably correctness is to
introduce IR and/or UV regularization, by finite-range forces and/or
softening.  IR: a periodic universe with, for example, Yukawa-type
potentials; this will enable modeling thermal equilibrium.  UV:
softening; which limits velocities and this allows small shared
timesteps.

Perhaps all we can ask for is to make a "pragmatically correct" code.
How to define this?  Perhaps we can define a Quality Number, as a type
of N-body version of a Linpack test, with large penalties for crashes.
You could run a code with N=1000 particles on 10 workstations for a
week, and in that way you could do 1000 runs past core collapse, in
order to gather statistics on accuracy-performance and crashes.
Or you could run 10^6 3-body resonant scatterings, following even the
longest-lasting systems; or you could run 10^4 5-body scatterings; etc.

AHA: we could construct an ACS wrapper around existing N-body codes.
That way, it would be much easier to test NBODYx, Kira, etc., and
compare the results to each other.  We could write simple scripts to
do whole series of runs and test for crashes and serious slow-downs.
This would be a nice tool for use in MODEST applications, and would
also be in the spirit of a Virtual Observatory approach.

-- trees

For what purposes might we want to use trees?

1. for making long-range force calculations more efficient

2. for treating perturbations on small-N clumps, and round-off

3. for dealing with black holes and other very massive objects that
   dominate perhaps a large fraction of the whole system

4. for dealing with unusual particles, such as high-speed particles.

Point 1 deals with the global problem, of switching from N to log N
in the number of force calculations per particle.  

AHA!  Points 2,3,4 deal with the local problem, on the level of mass,
position, and velocity: exactly the three physical ingredients of the
initial conditions of an N-body system.  This suggests that this list
might be reasonably complete!

* Tuesday, 2004-1-4

-- roundoff, density, and tidal forces

The criterion for avoiding roundoff is effectively a type of tidal
force constraint: a particle is spread out, effectively, over the
uncertainty in its position caused by roundoff.  Let us take a pair
of particles, with absolute position vector R (say, the average of the
two position vectors) and relative position vector r:

  Delta F_pair  <  epsilon ==>

  Delta F_pair = F_pair * (Delta F_pair / F_pair) =

                 (M / r^2) * (R / r) = (M / r^3) * R

where the first term in the last expression is the density obtained by
spreading out the heaviest of the two particles over their interparticle
sphere.  If that density is large compared to 1/R, we need to introduce
offset regularization.

-- six problems for clumps of interacting particles

Why do people use KS regularization?  There are at least five reasons,
four of which have nothing to do with KS!

On the level of force calculations:

1. for very high eccentricity, we need a special treatment, even in a
   c.o.m. frame, to integrate a parabolic orbit; here is where KS comes
   in, or some other trick, such as a reflection technique, used in Kira.

On the level of orbit integration, there are five more reasons:

2. speedup, provided by using "slow-KS" and collapsing binaries

3. accuracy: preventing the builtup of truncation errors in the
   internal orbit integration is one extra reason to use slow-KS and
   to collapse binaries

4. accuracy: prevent round-off errors through offset regularization

5. accuracy: allow stabilization of c.o.m. motion, through the use of
   binary trees.

-- improving binary trees to allow very heavy particles

Black holes can be treated much better, we think, with the following
improvement of the basic Kira binary tree algorithm.

Three rules.  Let "o" be a general node, where we use "^" below the
node to indicate that there may be many daughters, etc, attached below.

Two binary rules:

1) MERGE: when two nodes come too close, they merge:


                              o
     o    o        ==>       / \
     ^    ^                 o   o
                            ^   ^

2) SPLIT: when a node becomes too wide, it will split into two parts:

    
        o
       / \         ==>      o    o     
      o   o                 ^    ^     
      ^   ^

One trinary rule:

3) REARRANGE: when particle 1 has a much stronger attraction to its
   sister node than particle 2 below it, they swap places (M is a very
   massive particle):

          o                     o
         / \                   / \
        o   1      ==>        o   2
       / \                   / \
      M   2                 M   1

Another way to make these three diagrams is to use the following notation:

   -- a link between two nodes

   ...> a weak force exerted from one particle to another

   ---> a stronger force exerted from one particle to another

   ===> an even stronger force exerted from one particle to another

In these terms, we can write the three rules as follows:

1) MERGE:

       --->
     o  or  o      ==>     o --- o
       <---

2) SPLIT:

        --->
     o--and--o      ==>     o     o
        <---

3) REARRANGE:

          o                    o
         /                     |
      ^ / /                    |
     / or/_         ==>        |
    / / v  \_                  |
     /  ===> \_                |
    o    or    o         o-----------o 
        <===

At the end of the afternoon, we realized one big problem.

Our procedure will lead to very unbalanced trees, with a black hole at
the bottom of an almost linear chain of particles.  However, in the
standard way of calculating forces between particles in the Kira code,
this will lead, for k particles in the chain, to a calculation cost of
order k^3.

* Wednesday, 2004-1-5

We started our discussion with the realization that there is a
possible solution to the k^3 cost problem, that we ended with the
previous day.  It is possible to obtain a cost of order k^2, by
remembering partial results of force calculations, and storing them at
appropriate levels.  This would require one pass from the top down the
tree to the black hole, to calculate partial forces, followed by a
pass back up the tree.

We talked more about combining this new type of heavy-mass-adjusted
kira tree with a long-range force-speedup tree, and we concluded that
a Greengard-Roklin tree might be the most natural, after all.  We made
the following plan, to present under ACS:

-- shared timesteps

-- individual timesteps

-- block timesteps

-- Greengard-Rokhlin version of all that

-- then add a heavy-mass-adjusted kira tree

We found Greengard's thesis, as published in a book, and went through
that quickly, to remind ourselves of the overall structure of the G-R
tree implementation (the Fast Multipole Method).

* Thursday, 2004-1-6

-- grid codes

We spent quite a bit of time looking at many papers.  Especially
Skeel's papers were interesting, with a clear overall vision and
insight into the various aspects of the problem of classical molecular
dynamics simulations.

We concluded that a multiple grid method might be more appropriate
than a multipole expansion.  The reason is that the Fast Multipole
method still works with the whole 1/r^2 force, which diverges at
larges and short radii.  It would be better to make an extra
decomposition into various force components, as is done in Ewald
summation and in the Ahmad-Cohen method.

We investigated the possibility to use of order log N different force
components.  Each one would have a finite range, of 2^-k in natural
units.  The 1/r^2 force would be multiplied by a set of kernels.  The
largest-k f_k(x) kernel, for the longest forces, k=0, would behave like

  f_0(x) = A * x^3

which would convert the Kepler force near the origin into a harmonic
oscillator.  Beyond the largest range of the shorter forces, at:

  f_1(x) = 0  ==>  x = 1

we would require that the longest force would have full strength:

  f_0(x) = 1  for x = 1
 
We are free to pick the full functional form of f_0(x), as long as
it obeys the constraints of being cubic near the origin and unity for
x >= 1.  After we pick that functional form, we can construct f_k(x)
for higher k values, one by one, by insisting that

  f_k(x) = f_{k-1}(2x)  for  x <= 2^-k

and further to insist that the sum of all force term kernels equals unity,
everywhere.

-- general considerations: a 4-D tree

In hydrodynamics, space and time variations are coupled through the
Courant condition, basically a reflection of the fact that the space
and time dimensions are related by the speed of sound, just as in
relativity theory the speed of light unifies space and time into
spacetime.  In stellar dynamics, the virial theorem guarantees a
similar coupling, with the typical speed of particles being of order
unity in natural coordinates: this again is a type of speed-of-sound
criterion.

So unlike the molecular dynamics case, in stellar dynamics a grid can
form a natural extension of a block-timestep method, where forces or
the positions generating the forces are extrapolated and interpolated
in time, with time steps that are regulated by the smoothness of the
time variations.  So the same forces can be interpolated over space on
a grid with each mesh size of order of the scale of spatial variations
of the corresponding force component.

So the multigrid block timestep method is truly a 4D multigrid method,
that could be modular even in separating the treatment of
high-frequency and low-frequency contributions to the forces, or in
general in decomposing each force into log N forces, each for a
different frequency domain.

However, the virial theorem does not set a strict bound, the way the
Courant criterion does: there will always particles that either move
unusually fast or feel the high frequency components from other
unusually fast moving particles, even on relatively course scales; see
below.

-- grid structure and computational costs

The basic idea is to have k grids, where the first few grids cover all
of space, at least where the bulk of the particles are, while the
finer grids only appear in disconnected patches.  For each patch, we
would need at least 17 points in each direction, we estimated, in order
to have a good enough coverage along each coordinate axis, in both
positive and negative direction, to resolve f_k(x).  For 4th order
integration methods, 8 points in each, positive or negative, direction
migth be enough, leading to 17 points in total, but for higher order
schemes we may well need more points.

Note here that with any type of tree code, FMM or grid codes, going to
higher order in expansions means an exponential gain in accuracy; in
our case the highest accuracy attainable will increase exponentially
with the minimum number of grid points in each dimension.  This may
seem surprising, but it is related to the fact that doubling the
number of grid points allows you to double the order of the integrator,
leading to an error term "delta E" for which "log (delta E)" is twice
as small.

Anyway, this leads to the need to construct a grid patch with (17)^3
points, or about 5.10^3 points, around each point at the finer-mesh
grids.  Since building up a grid is rather cheap, mostly a matter of
spreading out mass over mesh points, and since solving Poisson's
equation then isn't that expensive either, a grid code of this type
may break even with an N^2 code for, say, N = 10^4 particles.
However, for smaller particle numbers it will be much more expensive
than direct integration codes.  While you could of course switch off
the grid for lower N, if perturbation schemes are based on these
grids, you may have to retain at least the finest meshes, for those
perturbations.  Still, the cost penalty might not be too bad.

After we discussed all this, we realized that it would not be possible
to assign a single value for the block time step to a single grip patch
at a give level k.  Instead, some particles will need a higher rate of
updates of their particle position than others.  We first thought to
subdivide grids at those places where needed, to accomodate fast
particles, as follows:

        ^   +---+---+---+---+
        |   |   |   |   |   |
        t   |   |   +---+---+
            |   |   |   |   |
            +---+---+---+---+
                        r -->

in order to satisfy the right-most particle below:


        ^   +*--+--*+-*-+---+
        |   |.  |  .|   |  .|
        t   | . |  .+--*+---+
            | . |  .|  .|   |
            +-*-+--*+---*---+
                        r -->

However, dealing with a non-homogeneous grid seemed to be too complex,
when you think about the details of the particle pushing implementations.

Instead, the most straightforward solution would be to introduce a
two-parameter family of subgrids: for each k value, the grids pertain
to a particular component of the force, but the particles on that grid
will be distributed over p subgrid, each of which has constant values
for their block time steps.

By this time we were ready to give up, at least for the time being.
It may still be a reasonable scheme, for very many particles, and we
should keep these ideas in mind, but for now, perhaps it is better to
get back to the world1.rb code, and to develop that further, more in
bottom-up rather than top-down fashion.

-- terrible triples

That evening we had a long discussion about the nature of three-body
systems in even low-N simulations.  As soon as you introduce
primordial binaries, you will form terrible triples.  Even for N=10^3
and in fact already for smaller N values, you will encounter two
types of situations where the integration will be very expensive:

1. even if you have a good criterion for determining when a terrible
   triple will be stable, there will always be cases that fall just
   near enough the stability boundary that you have to integrate it
   forever, simply because you don't know whether or not it will fall
   apart.

2. some terrible triples will have a very wide outer orbit, with a
   pericenter close enough to the inner orbit to count as a
   significant perturber for the inner orbit, and an apocenter large
   enough to let the outer orbit be perturbed essentially all the
   time, thus coupling the local and global force calculations in a
   way that cannot be easily separated.

There are serious questions here: can we ever guarantee that our
simulations will treat the cumulative effects of resonances, say,
correctly, if we introduce cost-cutting measures such as quarantining
triples when they are quite isolated?  Both the internal motions as
well as the very weak external perturbations could in principal lead
to such a triple growing unstable after all.

Well, we'll have to try to do the best we can, and then we should try
to measure the effects of various shortcuts, at least in a statistical
way.

* Friday, 2004-1-7

We decided to focus on getting kali1.rb working for up to N=128
particles, starting from the world1.rb version that we wrote in
October.  Here is the list of steps we plan to take:

-- implement general order Hermite, Aarseth, and collocation Runge-Kutta

-- define a good interface, to separate the integration schemes from the
rest of <tt>world1.rb</tt>

-- implement kira-type binary tree, at least one level deep

-- make a C speed-up version

-- calculate forces from worldline interactions (rather than snapshot)

-- start thinking about quadruple precision implementation

-- start thinking about implementing a perturber implementation

We plan to start our improvements on <tt>world1.rb</tt> in this order, and we
will call the new code <tt>kal1.rb</tt>.

