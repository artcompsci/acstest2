= A Two-Step Method

== A Matter of Memory

*Bob*: So far we have used only one-step methods: in each case we start
with the position and velocity at one point in time, in order to calculate
the position and velocity at the next time step.  The higher-order
schemes jump around a bit, to in-between times in case of the traditional
Runge-Kutta algorithms, or slightly before or beyond the one-step interval
in case of Yoshida's algorithms.  But even so, all of our schemes have been
self-starting.

As an alternative to jumping around, you can also remember the results from
a few earlier steps.  Fitting a polynomial to previous interaction calculations
will allow you to calculate higher time derivatives for the orbit . . . 

*Alice*: . . . wonderful!  Applause!

*Bob*: Huh?

*Alice*: You said it just right, using both the term _orbit_ and _interaction_
correctly.

*Bob*: What did I say?

*Alice*: You made the correct distinction between the physical interactions
on the right-hand side of the equations of motion, which we agreed to call
the interaction side, and the mathematical description of the orbit
characteristics on the left-hand side of the equations of motion,
which we decided to call the orbit side.

*Bob*: I guess your lessons were starting to sink in.  In any case, let me
put my equations where my mouth is, and let show the idea first for a
second-order multi-step scheme, a two-step scheme in fact.  We start with
the _orbit_ part, where we expand the acceleration in a Taylor series with
just one extra term:

:equation:
\label{firstacctaylor}
\ba(t) \,=\, \ba_0 + \bj_0 t + O(t^2)

*Alice*: Our old friend, the jerk, evaluated also at time <tex>$t=0$</tex>.

*Bob*: Speak for yourself, my friends are not jerks.  We can determine
the jerk at the beginning of our new time step if we can remember the
value of the acceleration at the beginning of the previous time step, at
time <tex>$t = -\Delta t$</tex>, as follows:

:equation:
\label{prevacc}
\ba_{-1} \,=\, \ba_0 - \bj_0 \Delta t  \,\,\,\,\Rightarrow

:equation:
\label{firstjerk}
\bj_0 \,=\, {1\over\Delta t}\, \left\{\ba_0 - \ba_{-1}\right\}

With this information, we can use the same approach as we did with the
forward Euler algorithm, but we can go one order higher in the Taylor
series expansion for the position and the velocity.  The forward Euler
method gave us:

:eqnarray:
\br_1 &=& \br_0 + \bv_0 \Delta t                       \nonumber\\
\bv_1 &=& \bv_0 + \ba_0 \Delta t                       \nonumber

but now we can write:

:eqnarray:
\label{ms2}
\br_1 &=& \br_0 + \bv_0 \Delta t + \hhalf \ba_0 (\Delta t)^2        \nonumber\\
\bv_1 &=& \bv_0 + \ba_0 \Delta t + \hhalf \bj_0 (\Delta t)^2

and we can use the value for the jerk given in Eq. (ref(firstjerk)),
without any need for half-steps, such as in the leapfrog or Runga-Kutta
methods.

A more direct, but less easily understandable way of writing these
equations is to substitute Eq. (ref(firstjerk)) into Eq. (ref(ms2)),
in order to get an equation written purely in terms of the accelerations
at different times:

:eqnarray:
\br_1 &=& \br_0 + \bv_0 \Delta t + \hhalf \ba_0 (\Delta t)^2        \nonumber\\
\bv_1 &=& \bv_0 + \hhalf\left(3\ba_0-\ba_1\right)\Delta t

I have seen this expression before, but I did not realize then that it
had such a simple explanation in terms of the jerk.

== Implementation

*Alice*: That is an interesting twist, and indeed a very different approach.

*Bob*: I have implemented this, starting from the file <tt>yo8body.rb</tt>,
and renaming it <tt>ms2body.rb</tt>.  I had to make one modification, though.
Previously, we kept track of the number of time steps with a counter +nsteps+
that was a local variable within the method +evolve+, but now we need that
information within the multi-step integrator, which I called <tt>ms2</tt>.

*Alice*: Why's that?

*Bob*: A multi-step method is not self-starting, since at the beginning of
the integration, there is no information about previous steps, simply because
there are no previous steps.  I solve this by borrowing another 2nd-order
method, <tt>rk2</tt> for the first time step.  But this means that <tt>ms2</tt>
has to know whether we are at the beginning of the integration or not, and
the easiest way to get that information there is to make the number of time
steps into an instance variable within the +Body+ class.

What I did was replace <tt>nsteps</tt> by <tt>@nsteps</tt>, in the two places
where it was used within the method +evolve+.  I made the same change in the
method <tt>write_diagnostics</tt>, which simplified matters, since before
we needed to pass <tt>nsteps</tt> as an argument to that method, and
now this is no longer necessary.

Here is the code:

 :inccode: .ms2body.rb+ms2

*Alice*: So the variable <tt>@prev_acc</tt> serves as your memory, to store
the previous acceleration.  At the start, when <tt>@nsteps</tt> is still
zero, you initialize <tt>@prev_acc</tt>, and during each next step, you
update that variable at the end, so that it always contains the value
of the acceleration at the _start_ of the previous step.

*Bob*: Yes.  The acceleration at the _end_ of the previous step would
be the same as the acceleration at the start of the current step, and
that value is stored in the local variable <tt>old_acc</tt>, as you can
see.  This allows me to calculate the jerk.

*Alice*: Or more precisely, the product of jerk <tex>$\bj$</tex>
and time step <tex>$\Delta t$</tex>, in the form of <tex>$\bj\Delta t$</tex>,
since it is only that combination that appears in Eq. (ref(ms2)), which
you write in the next two lines of code.  Okay, that is all clear!

*Bob*: When I first wrote this, I was wondering whether it was correct
to use this expression for the jerk, since strictly speaking, it gives
an approximation for the value of the jerk that is most accurate at a
time that is half a time step in the past, before the beginning of the
current time step:

<tex>$$
\bj_{-1/2} = \ba_0 -\ba_{-1}
$$</tex>

But then I realized that the difference does not matter, for a second
order integration scheme.  In terms of the next time derivative of
position, the snap <tex>$\bs = d\bj/dt$</tex>, the leading term of the
difference would be:

<tex>$$
\bj_0 = \bj_{-1/2} + \hhalf \bc_0 \Delta t
$$</tex>

All this would do is to add a term of order <tex>$(\Delta t)^3$</tex>
to the last line in Eq. (ref(ms2)), beyond the purview of a second-order
scheme.

*Alice*: Yes, from the point of view of a second-order integrator,
the jerk is simply constant, and we can evaluate it at whatever
point in time we want.

*Bob*: And before I forget, there is one more change I made,
in the file <tt>vector.rb</tt>, where I gave an extra method to our
+Vector+ class:

 :inccode: .vector.rb+-

*Alice*: Ah, yes, we only had addition and multiplication methods
before, but when you compute the jerk in <tt>ms2</tt> it is most
natural to use subtraction.

*Bob*: And for good measure, I added a division method as well:

 :inccode: .vector.rb+_divide_

*Alice*: I'm sure that will come in handy sooner or later.

== Testing

*Bob*: Let me show that the two-step integration method works.
I'll start again with a fourth-order Runge-Kutta result, as a
check:

 :commandoutput: ruby integrator_driver2j.rb < euler.in

Here is what the two-step method gives us:

 :commandoutput: ruby integrator_driver4a.rb < euler.in

Less accurate, but hey, it is only a second-order scheme, or so we hope.
Let's check:

 :commandoutput: ruby integrator_driver4b.rb < euler.in

*Alice*: It looks like 2nd order, but can you decrease the time step
by another factor of ten?

 :commandoutput: ruby integrator_driver4c.rb < euler.in

*Bob*: This makes it pretty clear: it is a second-order scheme.

== Predicting and Correcting

*Alice*: You have been talking about multi-step algorithms.
How does this relate to predictor-corrector methods?

*Bob*: It is the predictor part of a predictor-corrector scheme.
It is possible to squeeze some extra accuracy out of a multi-step
scheme, by using the information that you get at the end of a step,
to redo the last step a bit more accurately, `correcting' the step
you have just `predicted.'

In this procedure, you do _not_ increase the order of the integration
scheme.  So you only decrease the coefficient of the error, not its
dependence on time step length.  Still, if the coefficient is
sufficiently smaller, it may well be worthwhile to add a correction
step, in order to get this extra accuracy.

If you would like to know how these methods have been used historically,
you can read Sverre Aarseth's 2003 book <i>Gravitational N-Body
Simulations</i>.  He implemented a predictor-corrector scheme in the
early sixties, and his algorithm became the standard integration scheme
for collisional N-body calculations, during three decades.  It was only
with the invention of the Hermite scheme, by Jun makino, in the early
nineties, that we an alternative was offered.

*Alice*: I'd like to see a Hermite scheme implementation, too, in that
case.  As for the older scheme, is there an intuitive reason that the
corrector step would add more accuracy?

*Bob*: The key idea is that the predictor step involves an _extrapolation_,
while the corrector step involves an _interpolation_.  For the predictor
step, we are moving forwards, starting from the information at times
<tex>$t=-\Delta t$</tex> and <tex>$t=0$</tex>, to estimate the situation
at time <tex>$t=\Delta t$</tex>.  For the corrector step, effectively
what we do is to go back in time, from <tex>$t=\Delta t$</tex> back to
<tex>$t=0$</tex>.  Of course, we don't get precisely back to the same
position and velocity that we had before, so the difference between
the old and the new values is used to correct instead the position and
velocity at time <tex>$t=\Delta t$</tex>.

*Alice*: I see.  So the main point is that extrapolation
is inherently less accurate, for the same distance, as interpolation.

*Bob*: Exactly.  

*Alice*: Let me see whether I really got the idea, by writing it out
in equations.  The predicted position <tex>$\br_{p,1}$</tex> and
velocity <tex>$\bv_{p,1}$</tex> are determined as follows:

:eqnarray:
\br_{p,1} &=& \br_0 + \bv_0 \Delta t + \hhalf \ba_0 (\Delta t)^2    \nonumber\\
\bv_{p,1} &=& \bv_0 + \ba_0 \Delta t + \hhalf \bj_0 (\Delta t)^2

This is the extrapolation part.  If we now go back one time step,
we get the backward interpolation part, for which we compute our
reconstructed values <tex>$\tilde\br_0$</tex> and <tex>$\tilde\bv_0$</tex>
for the position and velocity at time <tex>$t=0$</tex> as:

:eqnarray:
\label{ms2back}
\tilde\br_0 &=& \br_{p,1} + \bv_{p,1}(-\Delta t) +
\hhalf \ba_{p,1}(-\Delta t)^2                                     \nonumber\\
\tilde\bv_0 &=& \bv_{p,1} + \ba_{p,1}(-\Delta t)
+ \hhalf \bj_{p,1}(-\Delta t)^2

where <tex>$\ba_{p,1}=\ba(\br_{p,1})$</tex> and where <tex>$\bj_{p,1}$</tex>
is the jerk obtained now from the last two values of the acceleration:

:equation:
\label{jerkp}
\bj_{p,1} \,=\, {1\over\Delta t}\, \left\{\ba_{p,1} - \ba_0\right\}

which is the same procedure that we used to obtain

<tex>$$
\bj_0 \,=\, {1\over\Delta t}\, \left\{\ba_0 - \ba_{-1}\right\}
$$</tex>

but just shifted by one unit in time.

If we now reinterpret Eq. (ref(ms2back)) as giving us _corrected_
values for the position and velocity at time <tex>$t=\Delta t$</tex>,
we have to shift <tex>$\tilde\br_0$</tex> and <tex>$\tilde\bv_0$</tex>
to <tex>$\br_0$</tex> and <tex>$\bv_0$</tex>, and apply that same
shift to the predicted quantities at time <tex>$t=\Delta t$</tex>.

As always, this sounds more complicated than it reads.  Let me write
it as equations.  First I express Eq. (ref(ms2back)) as an expression
for the predicted values in terms of the tilde values:

:eqnarray:
\br_{p,1} &=& \tilde\br_0 + \bv_{p,1}\Delta t
-\hhalf \ba_{p,1}(\Delta t)^2                                     \nonumber\\
\bv_{p,1} &=& \tilde\bv_0 + \ba_{p,1}\Delta t
- \hhalf \bj_{p,1}(\Delta t)^2

Then I shift the tilde values back to the original values at time
<tex>$t=0$</tex>, and in the process I should get the corrected
values for the position
<tex>$\br_{c,1}$</tex> and velocity <tex>$\bv_{c,1}$</tex> :

:eqnarray:
\label{ms2cwrong}
\br_{c,1} &=& \br_0 + \bv_{p,1} \Delta t - \hhalf \ba_{p,1} (\Delta t)^2
                                                                    \nonumber\\
\bv_{c,1} &=& \bv_0 + \ba_{p,1} \Delta t - \hhalf \bj_{p,1} (\Delta t)^2

In other words, starting with the corrected values for the differences
in position and velocity in the last time step, I shift everything in
such as way that we have:

:eqnarray:
\br_{c,1} - \br_{p,1} &=& \br_0 - \tilde\br_0    \nonumber\\
\bv_{c,1} - \bv_{p,1} &=& \bv_0 - \tilde\bv_0    \nonumber

But wait a minute, something is not quite optimal in Eq. (ref(ms2cwrong)).
The second line is expressed in terms of the best values we have for
all the quantities that occur there, but in the first line we can do better.
There is no reason to use the _predicted_ velocity at time <tex>$t=1$</tex>.
If we simply reverse the order in which we calculate those two lines, we can
instead use the _corrected_ version for the velocity at time <tex>$t=1$</tex>.
I mean:

:eqnarray:
\label{ms2c}
\bv_{c,1} &=& \bv_0 + \ba_{p,1} \Delta t - \hhalf \bj_{p,1} (\Delta t)^2
                                                                    \nonumber\\
\br_{c,1} &=& \br_0 + \bv_{c,1} \Delta t - \hhalf \ba_{p,1} (\Delta t)^2

*Bob*: Yes, this is exactly the procedure, with a clever twist at the end.

== An Old Friend

*Alice*: Well, shouldn't we implement this, too?

*Bob*: We may as well.  It shouldn't be hard.

*Alice*: So?  You look puzzled, still staring at the last set of equations.
Something wrong?

*Bob*: I'm bothered by something.  They just look too familiar.

*Alice*: What do you mean?

*Bob*: I've seen these before.  You know what?  I bet these are just
one way to express the leapfrog algorithm!

*Alice*: The leapfrog?  Do you think we have reinvented the leapfrog
algorithm in such a circuitous way?

*Bob*: I bet we just did.

*Alice*: It's not that implausible, actually: there are only so many
ways in which to write second-order algorithms.  As soon as you go
to fourth order and higher, there are many more possibilities, but
for second order you can expect some degeneracies to occur.

*Bob*: But before philosophizing further, let me see whether my hunch
is actually correct.  First I'll get rid of the predicted value for
the jerk, using Eq. (ref(jerkp)) in order to turn Eq. (ref(msc) into
expressions only in terms of the acceleration:

:eqnarray:
\label{almost}
\bv_{c,1} &=& \bv_0 + \hhalf \left( \ba_0 + \ba_{p,1} \right) \Delta t
                                                                    \nonumber\\
\br_{c,1} &=& \br_0 + \bv_{c,1} \Delta t - \hhalf \ba_{p,1} (\Delta t)^2

And hey, there we have our old friend the leapfrog already!

*Alice*: I guess your friendship with the frog is stronger than mine.
Can you show it to me more explicitly?

*Bob*: So that you can kiss to frog?

*Alice*: I'd rather not take my chances; I prefer to let it leap.

*Bob*: If you plug the expression for <tex>$\bv_{c,1}$</tex> in the
first line of Eq. (ref(almost)) back into the right-hand side of the
second line, you get:

:eqnarray:
\bv_{c,1} &=& \bv_0 + \hhalf \left( \ba_0 + \ba_{p,1} \right) \Delta t
                                                                    \nonumber\\
\br_{c,1} &=& \br_0 + \bv_0 \Delta t + \hhalf \ba_0 (\Delta t)^2

And this is _exactly_ how we started to implement the leapfrog, way
back when, with Eq. (ref(oldfriend)), which we wrote as:

:eqnarray:
{\bf r}_{i+1} & = & {\bf r}_i + {\bf v}_{i} \Delta t + 
{\bf a}_{i} (\Delta t)^2/2                                  \nonumber \\
{\bf v}_{i+1} & = & {\bf v}_i + ({\bf a}_i +
{\bf a}_{i+1})\Delta t / 2                                  \nonumber

*Alice*: I see, yes, no arguing with that.  Good!  This will save
us a bit of work, since now we don't have to implement a corrected
version of your method <tt>ms2</tt>, given that we already have it
in the code, in the form of the +leapfrog+ method.

*Bob*: Yes, apart from one very small detail: if we were to write a
corrected version of <tt>ms2</tt>, then the very first step would
still have to be a Runge-Kutta step, using <tt>rk2</tt>.  From there
on, we would _exactly_ follow the same scheme as the +leapfrog+ method.

But I agree, no need to implement that.
